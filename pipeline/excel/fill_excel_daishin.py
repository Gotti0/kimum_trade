import pandas as pd
import os
import sys
import re
import logging
import requests
import json
from datetime import datetime

# Import stock mapper
sys.path.append(os.getcwd())
try:
    from utils.stock_mapper import get_code_by_name
except ImportError as e:
    print(f"Error importing modules: {e}. Please ensure stock_mapper.py exists in the current directory.")
    sys.exit(1)

from utils.config import get_logger
from pipeline.excel.kiwoom_api_client import fetch_kiwoom_minute_data
from pipeline.excel.nasdaq_client import fetch_nasdaq_close

logger = get_logger("fill_excel_kiwoom_hybrid", "fill_excel_kiwoom.log")

def parse_date(date_str, current_year):
    """
    Parses 'M.D.', 'YY.M.D.', or 'YYYY.MM.DD.' string into int YYYYMMDD for comparison.
    Returns (yyyymmdd_int, updated_year, month_int)
    """
    if pd.isna(date_str):
        return None, current_year, None

    date_str = str(date_str).strip()
    match = re.match(r"(?:(\d{2}|\d{4})\.)?\s*(\d{1,2})\.\s*(\d{1,2})\.?", date_str)
    
    if match:
        year_str = match.group(1)
        month = int(match.group(2))
        day = int(match.group(3))
        
        year = current_year
        if year_str:
            if len(year_str) == 2:
                year = 2000 + int(year_str)
            else:
                year = int(year_str)
                
        return int(f"{year}{month:02d}{day:02d}"), year, month
    return None, current_year, None

def clean_price(value):
    try:
        if pd.isna(value):
            return None
        if isinstance(value, str):
            return abs(int(value.strip()))
        return abs(int(value))
    except Exception:
        return value

def add_minutes(time_int: int, minutes_to_add: int) -> int:
    """
    Adds or subtracts minutes from an HHMM integer format time.
    """
    hours = time_int // 100
    mins = time_int % 100
    
    total_mins = hours * 60 + mins + minutes_to_add
    
    new_hours = total_mins // 60
    new_mins = total_mins % 60
    
    return new_hours * 100 + new_mins

def detect_base_time(day_records):
    """
    Detects the starting time of the day by comparing early morning volume.
    We check the sum of volume from 09:00~09:05 vs 10:00~10:05 to account for
    delayed first-ticks (e.g. 09:02 first trade).
    """
    if not day_records:
        return 900, "9ì‹œ ì‹œì‘"
        
    vol_0900_window = 0
    vol_1000_window = 0
    
    for r in day_records:
        t = int(r['time'])
        if 900 <= t <= 905:
            vol_0900_window += int(r.get('volume', 0))
        elif 1000 <= t <= 1005:
            vol_1000_window += int(r.get('volume', 0))
            
    # ì •ê·œì¥(9ì‹œ)ì´ ì •ìƒì ìœ¼ë¡œ ì—´ë ¸ë‹¤ë©´ 09:00~09:05 ê±°ë˜ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤.
    # ìˆ˜ëŠ¥ ë“± 10ì‹œ ê°œì¥ì¼ì¸ ê²½ìš°ì—ë§Œ 10:00~10:05 ê±°ë˜ëŸ‰ì´ 9ì‹œ êµ¬ê°„ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ í½ë‹ˆë‹¤.
    if vol_1000_window > (vol_0900_window * 5) and vol_1000_window > 1000:
        return 1000, "10ì‹œ ì‹œì‘"
        
    # ë°ì´í„°ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë¶€ì¡±í•œ ê²½ìš° ì‹œì´ˆ ê±°ë˜ ì‹œê°„ì„ í™•ì¸í•©ë‹ˆë‹¤. (NXT 8ì‹œ ì œì™¸)
    if vol_0900_window == 0 and vol_1000_window == 0:
        for r in day_records:
            t = int(r['time'])
            if t >= 900:  # 8ì‹œ(NXT) ì œì™¸í•œ ì²« ì •ê·œ ê±°ë˜ì‹œê°„
                if t >= 1000 and t < 1100:
                    return 1000, "10ì‹œ ì‹œì‘"
                break
                
    return 900, "9ì‹œ ì‹œì‘"

def extract_time_points(minute_data, target_date_int, base_time: int):
    """
    Extracts the specific OHLC values for a given date relative to a base time.
    Extracts: 1, 2, 3, 4, 8, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30 mins after.
    Values are NOT forward filled; if data is missing, we leave it as None.
    """
    extracted = {}
    
    offsets = {
        1: "1ë¶„ì¢…ê°€", 2: "2ë¶„ì¢…ê°€", 3: "3ë¶„ì¢…ê°€", 4: "4ë¶„ì¢…ê°€",
        8: "8ë¶„ì¢…ê°€", 11: "11ë¶„ì¢…ê°€", 14: "14ë¶„ì¢…ê°€", 15: "15ë¶„ì¢…ê°€",
        16: "16ë¶„ì¢…ê°€", 17: "17ë¶„ì¢…ê°€", 18: "18ë¶„ì¢…ê°€", 19: "19ë¶„ì¢…ê°€",
        20: "20ë¶„ì¢…ê°€", 21: "21ë¶„ì¢…ê°€", 22: "22ë¶„ì¢…ê°€", 23: "23ë¶„ì¢…ê°€",
        24: "24ë¶„ì¢…ê°€", 25: "25ë¶„ì¢…ê°€", 26: "26ë¶„ì¢…ê°€", 29: "29ë¶„ì¢…ê°€",
        30: "30ë¶„ì¢…ê°€"
    }
    
    day_records = [row for row in minute_data if int(row['date']) == target_date_int]
    if not day_records:
         return extracted
         
    day_records = sorted(day_records, key=lambda x: int(x['time']))
    
def extract_time_points(minute_data, target_date_int, base_time: int):
    """
    Extracts the specific OHLC values for a given date relative to a base time.
    Extracts: 1, 2, 3, 4, 8, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30 mins after.
    Values are NOT forward filled; if data is missing, we leave it as None.
    """
    extracted = {}
    
    offsets = {
        1: "1ë¶„ì¢…ê°€", 2: "2ë¶„ì¢…ê°€", 3: "3ë¶„ì¢…ê°€", 4: "4ë¶„ì¢…ê°€",
        8: "8ë¶„ì¢…ê°€", 11: "11ë¶„ì¢…ê°€", 14: "14ë¶„ì¢…ê°€", 15: "15ë¶„ì¢…ê°€",
        16: "16ë¶„ì¢…ê°€", 17: "17ë¶„ì¢…ê°€", 18: "18ë¶„ì¢…ê°€", 19: "19ë¶„ì¢…ê°€",
        20: "20ë¶„ì¢…ê°€", 21: "21ë¶„ì¢…ê°€", 22: "22ë¶„ì¢…ê°€", 23: "23ë¶„ì¢…ê°€",
        24: "24ë¶„ì¢…ê°€", 25: "25ë¶„ì¢…ê°€", 26: "26ë¶„ì¢…ê°€", 29: "29ë¶„ì¢…ê°€",
        30: "30ë¶„ì¢…ê°€"
    }
    
    day_records = [row for row in minute_data if int(row['date']) == target_date_int]
    if not day_records:
         return extracted
         
    day_records = sorted(day_records, key=lambda x: int(x['time']))
    
    # Extract Open (first minute on or after base_time)
    valid_start_records = [r for r in day_records if int(r['time']) >= base_time]
    if valid_start_records:
        start_price = clean_price(valid_start_records[0]["open"])
    else:
        # Fallback to absolute first record
        start_price = clean_price(day_records[0]["open"])
        
    extracted["ì‹œì‘ê°€"] = start_price

    for offset, col_name in offsets.items():
        target_time = add_minutes(base_time, offset)
        
        # Look for the exact minute match. (No fallback)
        exact_record = [r for r in day_records if int(r['time']) == target_time]
        
        if exact_record:
            extracted[col_name] = clean_price(exact_record[0]["close"])
        else:
            extracted[col_name] = None
            
    return extracted

def extract_daily_ohlc(minute_data, target_date_int):
    """
    Extracts the daily High, Low, Open, Close values for a given date from 1-minute data.
    """
    day_records = [row for row in minute_data if int(row['date']) == target_date_int]
    if not day_records:
         return None
         
    day_records = sorted(day_records, key=lambda x: int(x['time']))
    
    open_p = clean_price(day_records[0]["open"])
    close_p = clean_price(day_records[-1]["close"])
    
    high_p = max(clean_price(row["high"]) for row in day_records if clean_price(row["high"]) is not None)
    low_p = min(clean_price(row["low"]) for row in day_records if clean_price(row["low"]) is not None)
    
    return {
        "ì‹œê°€": open_p,
        "ê³ ê°€": high_p,
        "ì €ê°€": low_p,
        "ì¢…ê°€": close_p
    }

def fill_excel_data(input_file):
    if not os.path.exists(input_file):
        logger.error(f"Input file not found: {input_file}")
        return

    logger.info(f"Processing Excel File: {input_file}...")
    
    try:
        df = pd.read_excel(input_file, header=0)
    except Exception as e:
        logger.error(f"Failed to read Excel: {e}")
        return

    # Handle typos
    for alt_name in ["ë‚ ì§œ", "ì‹¤ì œ", "ì¼ì", "ë‚ ì§œ "]:
        if alt_name in df.columns and "ë‚ ì" not in df.columns:
            df.rename(columns={alt_name: "ë‚ ì"}, inplace=True)
            break
            
    required_cols = ["ë‚ ì", "ì¢…ëª©", "ë‚ ì.1"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        logger.error(f"Missing base columns in Excel. Must contain: {required_cols}")
        return

    # Add "ì‹œì‘ì‹œê°„" column if not present
    if "ì‹œì‘ì‹œê°„" not in df.columns:
        # insert near the start (e.g., column index 2)
        df.insert(2, "ì‹œì‘ì‹œê°„", "")

    stock_data_cache = {}
    stock_range_cache = {}
    
    current_year = 2025
    prev_month = -1
    modified_count = 0

    for idx, row in df.iterrows():
        date_raw = row["ë‚ ì"]
        stock_name = row["ì¢…ëª©"]
        
        if pd.isna(date_raw) or pd.isna(stock_name):
            continue
            
        date_int, parsed_year, month = parse_date(date_raw, current_year)
        
        if parsed_year != current_year:
            current_year = parsed_year
        elif prev_month != -1 and month and month > prev_month:
            current_year -= 1
            date_int, _, _ = parse_date(date_raw, current_year)
            logger.info(f"Year rollback triggered! Now processing year: {current_year} at row {idx}")
            
        prev_month = month
        
        if not date_int:
            continue
            
        dt_ints = {}
        for offset, col_name in [(1, "ë‚ ì.1"), (2, "ë‚ ì.2"), (3, "ë‚ ì.3"), (4, "ë‚ ì.4"), (5, "ë‚ ì.5")]:
            if col_name in row and not pd.isna(row[col_name]):
                 dt_val, _, _ = parse_date(row[col_name], current_year)
                 dt_ints[offset] = dt_val

        # Get Stock Code
        raw_code = get_code_by_name(stock_name)
        if not raw_code:
            logger.warning(f"Stock map Code not found for '{stock_name}'. Skipping.")
            continue
            
        # We fetch Kiwoom regular data for all. If ATS, we fetch it with _NX flag. 
        # But Daishin cache and info fetch has been removed. 
        # Since the user requested "Kiwoom unified source", we use clean_cd.
        clean_cd = raw_code
        logger.info(f"Target Row {idx} [{date_int}] - {stock_name} ({clean_cd}) - Needs filling.")
        
        dates_needed = [date_int] + [dt for dt in dt_ints.values() if dt]
        min_date = min(dates_needed)
        max_date = max(dates_needed)
        
        if clean_cd not in stock_range_cache:
            stock_range_cache[clean_cd] = {"min": 99999999, "max": 0}
            
        checked_min = stock_range_cache[clean_cd]["min"]
        checked_max = stock_range_cache[clean_cd]["max"]
        
        needs_fetch = False
        if clean_cd not in stock_data_cache or min_date < checked_min or max_date > checked_max:
            needs_fetch = True
            
        if needs_fetch:
            try:
                max_dt = datetime.strptime(str(max_date), "%Y%m%d")
                from datetime import timedelta
                padded_max_dt = max_dt + timedelta(days=7)
                safe_base_date = int(padded_max_dt.strftime("%Y%m%d"))
            except Exception:
                safe_base_date = max_date
                 
            # Query Kiwoom directly.
            # We first try to fetch with _NX flag as fallback if standard returns nothing.
            fetched_data = fetch_kiwoom_minute_data(clean_cd, required_date_int=min_date, is_nxt=False, base_date_int=safe_base_date)
            
            if not fetched_data:
                logger.info(f"Row {idx}: No data for standard {clean_cd}. Trying _NX fallback.")
                fetched_data = fetch_kiwoom_minute_data(clean_cd, required_date_int=min_date, is_nxt=True, base_date_int=safe_base_date)
                
            if fetched_data:
                stock_data_cache[clean_cd] = fetched_data
            else:
                stock_data_cache[clean_cd] = [] 
                 
            stock_range_cache[clean_cd]["min"] = min(stock_range_cache[clean_cd]["min"], min_date)
            stock_range_cache[clean_cd]["max"] = max(stock_range_cache[clean_cd]["max"], max_date)
                 
        minute_data = stock_data_cache[clean_cd]
        
        nasdaq_col = "ë‚˜ìŠ¤ë‹¥ì¢…ê°€%" if "ë‚˜ìŠ¤ë‹¥ì¢…ê°€%" in df.columns else "ë‚˜ìŠ¤ë‹¥ì¢…ê°€"
        if 1 in dt_ints:
            nasdaq_close = fetch_nasdaq_close(dt_ints[1])
            if nasdaq_close is not None:
                if nasdaq_col not in df.columns:
                    df[nasdaq_col] = None
                df.at[idx, nasdaq_col] = nasdaq_close

        if not minute_data:
            logger.warning(f"No Data downloaded for {stock_name}. Cannot fill row {idx}.")
            continue
            
        # Detect Base Time automatically based on D+1 logic or D-day logic
        detect_day_int = dt_ints[1] if 1 in dt_ints else date_int
        day_records = [r for r in minute_data if int(r['date']) == detect_day_int]
        base_time, time_label = detect_base_time(sorted(day_records, key=lambda x: int(x['time'])))
        
        # Write the detected time label
        df.at[idx, "ì‹œì‘ì‹œê°„"] = time_label

        if date_int:
             d0_data = extract_daily_ohlc(minute_data, date_int)
             if d0_data:
                 if "ì‹œê°€" in df.columns: df.at[idx, "ì‹œê°€"] = d0_data["ì‹œê°€"]
                 if "ê³ ê°€" in df.columns: df.at[idx, "ê³ ê°€"] = d0_data["ê³ ê°€"]
                 if "ì €ê°€" in df.columns: df.at[idx, "ì €ê°€"] = d0_data["ì €ê°€"]
                 if "ì¢…ê°€" in df.columns: df.at[idx, "ì¢…ê°€"] = d0_data["ì¢…ê°€"]

        if 1 in dt_ints:
             if "ì¢…ëª©.1" in df.columns: df.at[idx, "ì¢…ëª©.1"] = clean_cd
                 
        if 1 in dt_ints:
            extracted = extract_time_points(minute_data, dt_ints[1], base_time)
            if extracted:
                for k, v in extracted.items():
                    if k in df.columns:
                        df.at[idx, k] = v
                        
            d1_data = extract_daily_ohlc(minute_data, dt_ints[1])
            if d1_data:
                if "ê³ ê°€.1" in df.columns: df.at[idx, "ê³ ê°€.1"] = d1_data["ê³ ê°€"]
                if "ì €ê°€" in df.columns: df.at[idx, "ì €ê°€"] = d1_data["ì €ê°€"]
                if "ì¢…ê°€.1" in df.columns: df.at[idx, "ì¢…ê°€.1"] = d1_data["ì¢…ê°€"]
                
        if 2 in dt_ints:
             d2_data = extract_daily_ohlc(minute_data, dt_ints[2])
             if d2_data:
                 if "ì‹œê°€.1" in df.columns: df.at[idx, "ì‹œê°€.1"] = d2_data["ì‹œê°€"]
                 if "ê³ ê°€.2" in df.columns: df.at[idx, "ê³ ê°€.2"] = d2_data["ê³ ê°€"]
                 if "ì €ê°€.1" in df.columns: df.at[idx, "ì €ê°€.1"] = d2_data["ì €ê°€"]
                 if "ì¢…ê°€.2" in df.columns: df.at[idx, "ì¢…ê°€.2"] = d2_data["ì¢…ê°€"]
                 
        if 3 in dt_ints:
             d3_data = extract_daily_ohlc(minute_data, dt_ints[3])
             if d3_data:
                 if "ì‹œê°€.2" in df.columns: df.at[idx, "ì‹œê°€.2"] = d3_data["ì‹œê°€"]
                 if "ê³ ê°€.3" in df.columns: df.at[idx, "ê³ ê°€.3"] = d3_data["ê³ ê°€"]
                 if "ì €ê°€.2" in df.columns: df.at[idx, "ì €ê°€.2"] = d3_data["ì €ê°€"]
                 if "ì¢…ê°€.3" in df.columns: df.at[idx, "ì¢…ê°€.3"] = d3_data["ì¢…ê°€"]

        if 4 in dt_ints:
             d4_data = extract_daily_ohlc(minute_data, dt_ints[4])
             if d4_data:
                 if "ì‹œê°€.3" in df.columns: df.at[idx, "ì‹œê°€.3"] = d4_data["ì‹œê°€"]
                 if "ê³ ê°€.4" in df.columns: df.at[idx, "ê³ ê°€.4"] = d4_data["ê³ ê°€"]
                 if "ì €ê°€.3" in df.columns: df.at[idx, "ì €ê°€.3"] = d4_data["ì €ê°€"]
                 if "ì¢…ê°€.4" in df.columns: df.at[idx, "ì¢…ê°€.4"] = d4_data["ì¢…ê°€"]
                 
        if 5 in dt_ints:
             d5_data = extract_daily_ohlc(minute_data, dt_ints[5])
             if d5_data:
                 if "ì‹œê°€.4" in df.columns: df.at[idx, "ì‹œê°€.4"] = d5_data["ì‹œê°€"]
                 if "ê³ ê°€.5" in df.columns: df.at[idx, "ê³ ê°€.5"] = d5_data["ê³ ê°€"]
                 if "ì €ê°€.4" in df.columns: df.at[idx, "ì €ê°€.4"] = d5_data["ì €ê°€"]
                 if "ì¢…ê°€.5" in df.columns: df.at[idx, "ì¢…ê°€.5"] = d5_data["ì¢…ê°€"]
            
        modified_count += 1
            
    output_excel = os.path.splitext(input_file)[0] + "_kiwoom_filled.xlsx"
    output_md = os.path.splitext(input_file)[0] + "_kiwoom_filled.md"
    
    try:
        df.to_excel(output_excel, index=False)
        logger.info(f"Saved filled Excel to {output_excel}")
        
        markdown_table = df.to_markdown(index=False)
        with open(output_md, 'w', encoding='utf-8') as f:
            f.write(markdown_table)
            
        logger.info(f"Saved filled Markdown to {output_md}")
        print(f"\nâœ… Processing Complete! Filled {modified_count} rows.")
        print(f"ğŸ“Š Result Excel: {output_excel}")
        print(f"ğŸ“ Result Markdown: {output_md}")
        
    except Exception as e:
        logger.error(f"Failed to save final outputs: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python fill_excel_daishin.py <input_excel_file>")
        sys.exit(1)
        
    target_file = sys.argv[1]
    fill_excel_data(target_file)
