import pandas as pd
import os
import sys
import re
import logging
import requests
import json
from datetime import datetime

# Import stock mapper
sys.path.append(os.getcwd())
try:
    from utils.stock_mapper import get_code_by_name
except ImportError as e:
    print(f"Error importing modules: {e}. Please ensure stock_mapper.py exists in the current directory.")
    sys.exit(1)

from utils.config import get_logger
from pipeline.excel.daishin_api_client import fetch_daishin_data, fetch_daishin_info, fetch_daishin_info_batch
from pipeline.excel.kiwoom_api_client import fetch_kiwoom_minute_data

logger = get_logger("fill_excel_kiwoom_hybrid", "fill_excel_kiwoom.log")

def parse_date(date_str, current_year):
    """
    Parses 'M.D.', 'YY.M.D.', or 'YYYY.MM.DD.' string into int YYYYMMDD for comparison.
    Returns (yyyymmdd_int, updated_year, month_int)
    """
    if pd.isna(date_str):
        return None, current_year, None

    date_str = str(date_str).strip()
    match = re.match(r"(?:(\d{2}|\d{4})\.)?\s*(\d{1,2})\.\s*(\d{1,2})\.?", date_str)
    
    if match:
        year_str = match.group(1)
        month = int(match.group(2))
        day = int(match.group(3))
        
        year = current_year
        if year_str:
            if len(year_str) == 2:
                year = 2000 + int(year_str)
            else:
                year = int(year_str)
                
        return int(f"{year}{month:02d}{day:02d}"), year, month
    return None, current_year, None

def clean_price(value):
    try:
        if pd.isna(value):
            return None
        if isinstance(value, str):
            return abs(int(value.strip()))
        return abs(int(value))
    except Exception:
        return value

def add_minutes(time_int: int, minutes_to_add: int) -> int:
    """
    Adds or subtracts minutes from an HHMM integer format time.
    Ex: add_minutes(900, -3) -> 857
    Ex: add_minutes(859, 2) -> 901
    """
    hours = time_int // 100
    mins = time_int % 100
    
    total_mins = hours * 60 + mins + minutes_to_add
    
    new_hours = total_mins // 60
    new_mins = total_mins % 60
    
    return new_hours * 100 + new_mins


def extract_time_points(minute_data, target_date_int, base_time: int):
    """
    Extracts the specific OHLC values for a given date relative to a base time.
    base_time: e.g., 800 (8:00), 900 (9:00), 1000 (10:00).
    Extracts: 1, 2, 3, 4, 8, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 30 mins after.
    """
    extracted = {}
    
    # Offsets required: (mins_after)
    offsets = {
        1: "1ë¶„ì¢…ê°€",
        2: "2ë¶„ì¢…ê°€",
        3: "3ë¶„ì¢…ê°€",
        4: "4ë¶„ì¢…ê°€",
        8: "8ë¶„ì¢…ê°€",
        11: "11ë¶„ì¢…ê°€",
        14: "14ë¶„ì¢…ê°€",
        15: "15ë¶„ì¢…ê°€",
        16: "16ë¶„ì¢…ê°€",
        17: "17ë¶„ì¢…ê°€",
        18: "18ë¶„ì¢…ê°€",
        19: "19ë¶„ì¢…ê°€",
        20: "20ë¶„ì¢…ê°€",
        21: "21ë¶„ì¢…ê°€",
        22: "22ë¶„ì¢…ê°€",
        23: "23ë¶„ì¢…ê°€",
        24: "24ë¶„ì¢…ê°€",
        25: "25ë¶„ì¢…ê°€",
        26: "26ë¶„ì¢…ê°€",
        29: "29ë¶„ì¢…ê°€",
        30: "30ë¶„ì¢…ê°€"
    }
    
    # Filter records for the specific date
    day_records = [row for row in minute_data if int(row['date']) == target_date_int]
    
    if not day_records:
         return extracted
         
    # ì •ë ¬
    day_records = sorted(day_records, key=lambda x: int(x['time']))
    
    # ì‹œì‘ê°€(Open) í• ë‹¹ (í•´ë‹¹ì¼ ì „ì²´ ë°ì´í„° ì¤‘ ê°€ì¥ ì²« ë°ì´í„°ì˜ ì‹œê°€ ì‚¬ìš© - base_time ê·¼ì ‘ ë°ì´í„° ìš°ì„ , ì•„ë‹ˆë©´ ì•ˆì „í•˜ê²Œ ê·¸ëƒ¥ ì²« ë°ì´í„°)
    # ì¡°ê¸ˆ ë” ì•ˆì „í•˜ê²Œ base_time ì´í›„ì˜ ì²« ê±°ë˜ ë°ì´í„°ë¥¼ ì‹œê°€ë¡œ ì¡ìŒ (VI ë“±ìœ¼ë¡œ 9ì‹œ 2ë¶„ ì‹œì‘ ì‹œ ëŒ€ì‘)
    valid_start_records = [r for r in day_records if int(r['time']) >= base_time]
    if valid_start_records:
        extracted["ì‹œì‘ê°€"] = clean_price(valid_start_records[0]["open"])
    else:
        extracted["ì‹œì‘ê°€"] = clean_price(day_records[0]["open"]) # Fallback

    # ê° offsetì— í•´ë‹¹í•˜ëŠ” ì¢…ê°€ ì¶”ì¶œ
    for offset, col_name in offsets.items():
        target_time = add_minutes(base_time, offset)
        
        # target_time ì´í•˜ì˜ ê°€ì¥ ìµœê·¼ ë¶„ë´‰ ë°ì´í„° ê²€ìƒ‰ (ê²°ì¸¡ì¹˜ ë°©ì–´ë¡œ ì´ì „ ë¶„ë´‰ ì¢…ê°€ ëŒê³ ì˜¤ê¸°)
        valid_rows = [r for r in day_records if int(r['time']) <= target_time]
        
        if valid_rows:
            # day_recordsëŠ” ì´ë¯¸ ì‹œê°„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ì´ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë§ˆì§€ë§‰ ì›ì†Œê°€ ê°€ì¥ ê°€ê¹Œìš´ ê³¼ê±° ë°ì´í„°
            extracted[col_name] = clean_price(valid_rows[-1]["close"])
        else:
            extracted[col_name] = None # í•´ë‹¹ ì‹œê°„ ì´ì „ì— ê±°ë˜ ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
            
    return extracted

def extract_daily_ohlc(minute_data, target_date_int):
    """
    Extracts the daily High, Low, Open, Close values for a given date from 1-minute data.
    """
    # Filter records for the specific date
    day_records = [row for row in minute_data if int(row['date']) == target_date_int]
    
    if not day_records:
         return None
         
    day_records = sorted(day_records, key=lambda x: int(x['time']))
    
    open_p = clean_price(day_records[0]["open"])
    close_p = clean_price(day_records[-1]["close"])
    
    high_p = max(clean_price(row["high"]) for row in day_records if clean_price(row["high"]) is not None)
    low_p = min(clean_price(row["low"]) for row in day_records if clean_price(row["low"]) is not None)
    
    return {
        "ì‹œê°€": open_p,
        "ê³ ê°€": high_p,
        "ì €ê°€": low_p,
        "ì¢…ê°€": close_p
    }

def fill_excel_data(input_file):
    if not os.path.exists(input_file):
        logger.error(f"Input file not found: {input_file}")
        return

    logger.info(f"Processing Excel File: {input_file}...")
    
    try:
        # Assuming header is at the top row (header=0)
        df = pd.read_excel(input_file, header=0)
    except Exception as e:
        logger.error(f"Failed to read Excel: {e}")
        return

    # Handle typos or alternative names for date column
    for alt_name in ["ë‚ ì§œ", "ì‹¤ì œ", "ì¼ì", "ë‚ ì§œ "]:
        if alt_name in df.columns and "ë‚ ì" not in df.columns:
            df.rename(columns={alt_name: "ë‚ ì"}, inplace=True)
            break
            
    # Get required columns mapped safely
    # In pandas with duplicate names: 'ë‚ ì.1', 'ì‹œê°€', 'ê³ ê°€' etc.
    # To check easily, we don't strictly require ALL 51 columns but we shouldn't crash.
    required_cols = ["ë‚ ì", "ì¢…ëª©", "ë‚ ì.1"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        logger.error(f"Missing base columns in Excel. Must contain: {required_cols}")
        return

    # =============== [PHASE 3 OPTIMIZATION] PRE-FETCH COMPANY INFO ===============
    logger.info("Extracting unique stock codes for batch info pre-fetching...")
    unique_stocks = df["ì¢…ëª©"].dropna().unique()
    unique_daishin_codes = []
    
    for stock_name in unique_stocks:
        raw_code = get_code_by_name(stock_name)
        if raw_code:
            unique_daishin_codes.append(f"A{raw_code}")
            
    # Deduplicate again just in case
    unique_daishin_codes = list(set(unique_daishin_codes))
    
    company_info_cache = {}
    CHUNK_SIZE = 200
    
    if unique_daishin_codes:
        logger.info(f"Initiating batch fetch for {len(unique_daishin_codes)} unique stocks in {len(unique_daishin_codes)//CHUNK_SIZE + 1} chunks.")
        
        for i in range(0, len(unique_daishin_codes), CHUNK_SIZE):
            chunk = unique_daishin_codes[i:i+CHUNK_SIZE]
            batch_result = fetch_daishin_info_batch(chunk)
            if batch_result:
                 company_info_cache.update(batch_result)
                 
        logger.info(f"Successfully pre-fetched company info for {len(company_info_cache)} stocks. Proceeding to main loop.")
    # =================================================================================

    # Cache downloaded stock data to avoid re-fetching the same stock for different days
    stock_data_cache = {}
    
    current_year = 2025 # Starting year from top of file
    prev_month = -1
    modified_count = 0

    for idx, row in df.iterrows():
        date_raw = row["ë‚ ì"]
        stock_name = row["ì¢…ëª©"]
        
        if pd.isna(date_raw) or pd.isna(stock_name):
            continue
            
        # Parse Date & Handle Year Rollback
        date_int, parsed_year, month = parse_date(date_raw, current_year)
        
        if parsed_year != current_year:
            current_year = parsed_year
        elif prev_month != -1 and month and month > prev_month:
            # e.g., prev was 1 (Jan), now 12 (Dec) -> Year decreased
            current_year -= 1
            date_int, _, _ = parse_date(date_raw, current_year)
            logger.info(f"Year rollback triggered! Now processing year: {current_year} at row {idx}")
            
        prev_month = month
        
        if not date_int:
            continue
            
        # Extract target offset dates (ë‚ ì.1 to ë‚ ì.5)
        dt_ints = {}
        for offset, col_name in [(1, "ë‚ ì.1"), (2, "ë‚ ì.2"), (3, "ë‚ ì.3"), (4, "ë‚ ì.4"), (5, "ë‚ ì.5")]:
            if col_name in row and not pd.isna(row[col_name]):
                 # use the *current* context year for these lookaheads.
                 # (simplification: assume they are purely lookaheads within the same/next month)
                 dt_val, _, _ = parse_date(row[col_name], current_year)
                 dt_ints[offset] = dt_val

        # Get Stock Code
        raw_code = get_code_by_name(stock_name)
        if not raw_code:
            logger.warning(f"Stock map Code not found for '{stock_name}'. Skipping.")
            continue
            
        daishin_code = f"A{raw_code}"
        
        logger.info(f"Target Row {idx} [{date_int}] - {stock_name} ({daishin_code}) - Needs filling.")
        
        # 1-1. Fill Company Info (MarketCap, Sector, ATS, Market) for EVERY row
        info_data = company_info_cache.get(daishin_code)
        
        # Fallback to individual fetch if it failed in batch
        if not info_data:
            info_data = fetch_daishin_info(daishin_code)
            if info_data:
                company_info_cache[daishin_code] = info_data # ìºì‹œì— ì €ì¥í•˜ì—¬ ë‹¤ìŒ ë™ì¼ ì¢…ëª©ì—ì„œ ì¬ì‚¬ìš©
                
        if info_data:
            if "(ì–µì›)" in df.columns:
                df.at[idx, "(ì–µì›)"] = info_data.get("MarketCap")
            if "ì—…ì¢…" in df.columns:
                sector_val = info_data.get("Sector")
                if isinstance(sector_val, str):
                    # ë¶ˆí•„ìš”í•œ ê±°ë˜ì†Œ ì •ë³´ ì ‘ë‘ì–´ ì œê±° (ì˜ˆ: 'ì½”ìŠ¤ë‹¥ ê¸°ê³„' -> 'ê¸°ê³„')
                    sector_val = re.sub(r'^(?:ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ì½”ë„¥ìŠ¤|KOSPI|KOSDAQ)\s*', '', sector_val).strip()
                df.at[idx, "ì—…ì¢…"] = sector_val
            
            # Set MarketType (usually E column, 'Unnamed: 4')
            if "Unnamed: 4" in df.columns:
                df.at[idx, "Unnamed: 4"] = info_data.get("MarketType")
                
            # ATS (F column) might be missing a header or we add it explicitly.
            if "ëŒ€ì²´ê±°ë˜ì†Œ" not in df.columns:
                df.insert(5, "ëŒ€ì²´ê±°ë˜ì†Œ", "") # Insert at F (index 5)
            df.at[idx, "ëŒ€ì²´ê±°ë˜ì†Œ"] = info_data.get("ATS_Nextrade")

        # NXT ì¢…ëª© íŒë³„ (Y/N)
        ats_val = str(df.at[idx, "ëŒ€ì²´ê±°ë˜ì†Œ"]).strip().upper()

        # 1-2. Fetch Minute Chart Data (from cache or API)
        if daishin_code not in stock_data_cache:
             # We determine the exact date range needed including D+1~D+5 lookaheads
             dates_needed = [date_int] + [dt for dt in dt_ints.values() if dt]
             min_date = min(dates_needed)
             max_date = max(dates_needed)
             
             # Pad max_date by ~7 days to ensure consecutive rows in the same spreadsheet are covered
             try:
                 max_dt = datetime.strptime(str(max_date), "%Y%m%d")
                 from datetime import timedelta
                 padded_max_dt = max_dt + timedelta(days=7)
                 safe_base_date = int(padded_max_dt.strftime("%Y%m%d"))
             except Exception:
                 safe_base_date = max_date
                 
             is_nxt = (ats_val == "Y")
             if is_nxt:
                 fetched_data = fetch_kiwoom_minute_data(daishin_code, required_date_int=min_date, is_nxt=is_nxt, base_date_int=safe_base_date)
             else:
                 fetched_data = fetch_daishin_data(daishin_code, required_date_int=min_date)
                 
             if fetched_data:
                 stock_data_cache[daishin_code] = fetched_data
             else:
                 stock_data_cache[daishin_code] = [] # Mark as failed/empty to avoid re-spamming API
                 
        minute_data = stock_data_cache[daishin_code]
        
        if not minute_data:
            logger.warning(f"No Data downloaded for {stock_name}. Cannot fill row {idx}.")
            continue
            
        # 2. Base Time ê²°ì • ë¡œì§
        # 10ì‹œ ì‹œì‘ ê²€ì‚¬: ë³´í†µ ì»¬ëŸ¼ 6(Gì—´)ì— ì…ë ¥ë˜ë¯€ë¡œ ì „ì²´ row ê°’ì„ í™•ì¸
        is_10_am_start = False
        for val in row.values:
            if isinstance(val, str) and "10ì‹œì‹œì‘" in val.replace(" ", ""):
                is_10_am_start = True
                break
        
        base_time = 900 # ê¸°ë³¸ KRX ì‹œì‘ì‹œê°„ (09:00)
        
        if is_10_am_start:
            base_time = 1000 # 10:00 ìˆ˜ëŠ¥ ë“± ì§€ì—°ê°œì¥
        elif ats_val == "Y":
            base_time = 800  # 08:00 NXT ì˜¤í”ˆ
            
        # 3. Fallback ë¡œì§ (ê³¼ê±° ë°ì´í„° ë¶€ì¡± ëŒ€ì‘)
        # NXT ì¢…ëª©('Y')ì´ë¼ 800ì„ ì„¤ì •í–ˆìœ¼ë‚˜, í•´ë‹¹ ë‚ ì§œì˜ ì²« ë¶„ë´‰ì´ 8ì‹œ 50ë¶„ ì´í›„ì¸ ê²½ìš°(KRX ì‹œì ˆ ë°ì´í„°)
        if base_time == 800 and 1 in dt_ints:
            day_records = [r for r in minute_data if int(r['date']) == dt_ints[1]]
            if day_records:
                first_time = min((int(r['time']) for r in day_records))
                if first_time >= 850:
                    logger.info(f"Row {idx}: ATS='Y' but first time for {dt_ints[1]} is {first_time}. Falling back to KRX base_time 900.")
                    base_time = 900

        # 4. Process D-day (A column date)
        if date_int:
             d0_data = extract_daily_ohlc(minute_data, date_int)
             if d0_data:
                 if "ì‹œê°€" in df.columns: df.at[idx, "ì‹œê°€"] = d0_data["ì‹œê°€"]
                 if "ê³ ê°€" in df.columns: df.at[idx, "ê³ ê°€"] = d0_data["ê³ ê°€"]
                 if "ì €ê°€" in df.columns: df.at[idx, "ì €ê°€"] = d0_data["ì €ê°€"]
                 if "ì¢…ê°€" in df.columns: df.at[idx, "ì¢…ê°€"] = d0_data["ì¢…ê°€"]

        # 4-1. Process NXT (which uses D+1 date, but only 'ì¢…ëª©.1' is strictly necessary here since OHLC is handled below)
        if 1 in dt_ints:
             if "ì¢…ëª©.1" in df.columns: df.at[idx, "ì¢…ëª©.1"] = daishin_code
                 
        # 5. Process Minute Data and Daily Data for D+1 (ë‚ ì.1)
        if 1 in dt_ints:
            extracted = extract_time_points(minute_data, dt_ints[1], base_time)
            if extracted:
                for k, v in extracted.items():
                    if k in df.columns:
                        df.at[idx, k] = v
                        
            # D+1 Daily data is mapped to "ê³ ê°€.1", "ì €ê°€", "ì¢…ê°€.1"
            d1_data = extract_daily_ohlc(minute_data, dt_ints[1])
            if d1_data:
                if "ê³ ê°€.1" in df.columns: df.at[idx, "ê³ ê°€.1"] = d1_data["ê³ ê°€"]
                if "ì €ê°€" in df.columns: df.at[idx, "ì €ê°€"] = d1_data["ì €ê°€"]
                if "ì¢…ê°€.1" in df.columns: df.at[idx, "ì¢…ê°€.1"] = d1_data["ì¢…ê°€"]
                
        # 4. Process D+2 (ë‚ ì.2) -> ì‹œê°€.1, ê³ ê°€.2, ì €ê°€.1, ì¢…ê°€.2
        if 2 in dt_ints:
             d2_data = extract_daily_ohlc(minute_data, dt_ints[2])
             if d2_data:
                 if "ì‹œê°€.1" in df.columns: df.at[idx, "ì‹œê°€.1"] = d2_data["ì‹œê°€"]
                 if "ê³ ê°€.2" in df.columns: df.at[idx, "ê³ ê°€.2"] = d2_data["ê³ ê°€"]
                 if "ì €ê°€.1" in df.columns: df.at[idx, "ì €ê°€.1"] = d2_data["ì €ê°€"]
                 if "ì¢…ê°€.2" in df.columns: df.at[idx, "ì¢…ê°€.2"] = d2_data["ì¢…ê°€"]
                 
        # 5. Process D+3 (ë‚ ì.3) -> ì‹œê°€.2, ê³ ê°€.3, ì €ê°€.2, ì¢…ê°€.3
        if 3 in dt_ints:
             d3_data = extract_daily_ohlc(minute_data, dt_ints[3])
             if d3_data:
                 if "ì‹œê°€.2" in df.columns: df.at[idx, "ì‹œê°€.2"] = d3_data["ì‹œê°€"]
                 if "ê³ ê°€.3" in df.columns: df.at[idx, "ê³ ê°€.3"] = d3_data["ê³ ê°€"]
                 if "ì €ê°€.2" in df.columns: df.at[idx, "ì €ê°€.2"] = d3_data["ì €ê°€"]
                 if "ì¢…ê°€.3" in df.columns: df.at[idx, "ì¢…ê°€.3"] = d3_data["ì¢…ê°€"]

        # 6. Process D+4 (ë‚ ì.4) -> ì‹œê°€.3, ê³ ê°€.4, ì €ê°€.3, ì¢…ê°€.4
        if 4 in dt_ints:
             d4_data = extract_daily_ohlc(minute_data, dt_ints[4])
             if d4_data:
                 if "ì‹œê°€.3" in df.columns: df.at[idx, "ì‹œê°€.3"] = d4_data["ì‹œê°€"]
                 if "ê³ ê°€.4" in df.columns: df.at[idx, "ê³ ê°€.4"] = d4_data["ê³ ê°€"]
                 if "ì €ê°€.3" in df.columns: df.at[idx, "ì €ê°€.3"] = d4_data["ì €ê°€"]
                 if "ì¢…ê°€.4" in df.columns: df.at[idx, "ì¢…ê°€.4"] = d4_data["ì¢…ê°€"]
                 
        # 7. Process D+5 (ë‚ ì.5) -> ì‹œê°€.4, ê³ ê°€.5, ì €ê°€.4, ì¢…ê°€.5
        if 5 in dt_ints:
             d5_data = extract_daily_ohlc(minute_data, dt_ints[5])
             if d5_data:
                 if "ì‹œê°€.4" in df.columns: df.at[idx, "ì‹œê°€.4"] = d5_data["ì‹œê°€"]
                 if "ê³ ê°€.5" in df.columns: df.at[idx, "ê³ ê°€.5"] = d5_data["ê³ ê°€"]
                 if "ì €ê°€.4" in df.columns: df.at[idx, "ì €ê°€.4"] = d5_data["ì €ê°€"]
                 if "ì¢…ê°€.5" in df.columns: df.at[idx, "ì¢…ê°€.5"] = d5_data["ì¢…ê°€"]
            
        modified_count += 1
            
    # Save Results
    output_excel = os.path.splitext(input_file)[0] + "_kiwoom_filled.xlsx"
    output_md = os.path.splitext(input_file)[0] + "_kiwoom_filled.md"
    
    try:
        df.to_excel(output_excel, index=False)
        logger.info(f"Saved filled Excel to {output_excel}")
        
        # Also save MD
        markdown_table = df.to_markdown(index=False)
        with open(output_md, 'w', encoding='utf-8') as f:
            f.write(markdown_table)
            
        logger.info(f"Saved filled Markdown to {output_md}")
        print(f"\nâœ… Processing Complete! Filled {modified_count} rows.")
        print(f"ğŸ“Š Result Excel: {output_excel}")
        print(f"ğŸ“ Result Markdown: {output_md}")
        
    except Exception as e:
        logger.error(f"Failed to save final outputs: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python fill_excel_daishin.py <input_excel_file>")
        sys.exit(1)
        
    target_file = sys.argv[1]
    fill_excel_data(target_file)
